{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I've gone ahead and used \"=\" in place of \"<-\" because the latest R doesn't\n",
    "# care, also makes the code more (Pythonic)readable IMHO\n",
    "\n",
    "\n",
    "# Legend of MAGIC Numbers, File Paths and Arguments\n",
    "CORRELATION_STRATEGY = \"pearson\"\n",
    "PATH_TO_DATA = \"~/Git-Projects/spect-team/Data/\"\n",
    "PRIMARY_FILE = \"Data_Adults_1.csv\"\n",
    "INFO_START = 2\n",
    "INFO_END = 14\n",
    "DISORDER_START = 15\n",
    "DISORDER_END = 76\n",
    "SURVEY_START = 77\n",
    "SURVEY_END = 378\n",
    "RCBF_RAW_START = 379\n",
    "RCBF_RAW_END = 636\n",
    "RCBF_SCALED_START = 637\n",
    "RCBF_SCALED_END = 754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above describe some Magic numbers and column indices. They offer very rudimentary insight into what columns map on to what data. While we're at it let us also define a handful of helpful functions. \n",
    "**PLEASE FIX THE FILE PATH TO SOMETHING THAT POINTS TO A FILE ON YOUR COMPUTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes Correlation Between 2 quantities and returns a correlation vector.\n",
    "# Param: quant_a : The first data frame to correlate\n",
    "# Param: quant_b : The second data frame to correlate with \n",
    "# Return:  A vector of column correlation\n",
    "correlate = function(quant_a, quant_b = NA) {\n",
    "    if (is.na(quant_b)){\n",
    "        cor_val = cor(quant_a, method = CORRELATION_STRATEGY)\n",
    "    } else {\n",
    "        cor_val = cor(quant_a, quant_b, method = CORRELATION_STRATEGY)\n",
    "    }\n",
    "    return (cor_val)\n",
    "}\n",
    "\n",
    "\n",
    "# This function returns a matrix with 2 rows, one with n smallest values and the other with n largest.\n",
    "# Param: vec : The vector from which the values need to be extracted\n",
    "# Param: n : The number of values needed\n",
    "# Param: req_plot : A switch which will optionally plot the data if required\n",
    "# Return: A row bound matrix of n smallest and largest values respectively\n",
    "sort_plot = function(vec, n, req_plot = FALSE) {\n",
    "    smallest_n = sort(vec)[1:n]\n",
    "    largest_n = sort(vec, TRUE)[1:n]\n",
    "    if (req_plot) {\n",
    "        plot(vec)\n",
    "    }\n",
    "    return (rbind(smallest_n, largest_n))\n",
    "}\n",
    "\n",
    "\n",
    "# Function prefixes the patient ID column to a data fram for kicks, actually it\n",
    "# does this so that we can find out who is where.\n",
    "# Param: start_index : The beginning of the columns to bind the patient ID\n",
    "# ahead of\n",
    "# Param: end_index : The end of the columns\n",
    "# Return: a data frame with the required columns prefixed by patient ID \n",
    "prefix_id = function(start_index, end_index) {\n",
    "    return (cbind(cleansed_data[2], cleansed_data[, start_index : end_index]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Question - 1\n",
    "What is the size of our training data? What is the length of each feature vector and how many features does each vector have?\n",
    "\n",
    "### Descriptive Question - 2\n",
    "What does the presence of Null values indicate? How should they be dealt with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset as is has 7674 rows and 754 columns\n",
      "The downsampled dataset, omitting rows that have NA values has 2796 rows and 754 columns\n"
     ]
    }
   ],
   "source": [
    "spect_data = data.frame(read.csv(paste0(PATH_TO_DATA, PRIMARY_FILE), as.is = TRUE))\n",
    "cleansed_data = na.omit(spect_data) # deal with Null values\n",
    "cat(\"The dataset as is has\", nrow(spect_data), \"rows and\", ncol(spect_data), \"columns\\n\")\n",
    "cat(\"The downsampled dataset, omitting rows that have NA values has\", nrow(cleansed_data), \n",
    "    \"rows and\", ncol(cleansed_data), \"columns\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1**: As evidenced by the code above even when we elect to throw out any row that has a NA value we still have a sizeable datset. This doesn't mean however that we will not use the data thrown out. We will simply downsample till we have a complete yet sizeable dataset and draw corellation conclusions etc. from it. Once we have these theories we will try to apply them to less downsampled datasets to see if we can fill in missing data and verify if our theories still hold.\n",
    "\n",
    "**Answer 2**: As stated in the answer above, NA values were thrown out. We still had a considerably large dataset after doing so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Question - 4\n",
    "What do the features in the vector indicate? What are they for? How are the feature values arrived at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleansed_data_patient_info = cleansed_data[,INFO_START : INFO_END]\n",
    "cleansed_data_disorder_info = prefix_id(DISORDER_START, DISORDER_END)\n",
    "cleansed_data_survey_info = prefix_id(SURVEY_START, SURVEY_END)\n",
    "cleansed_data_RCBF_raw_info = prefix_id(RCBF_RAW_START, RCBF_RAW_END)\n",
    "cleansed_data_RCBF_scaled_info = prefix_id(RCBF_SCALED_START, RCBF_SCALED_END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4**: By manually examining the data, we realized that certain portions of the dataset contain specific data that can be\n",
    "examined mutually exclusively from the other portions of the dataset. We made note of the column indices that allow the data to be split into these mutually exclusive portions and went ahead and split the data set according to those indices. This also allows for easier correlation experiments.\n",
    "\n",
    "- The columns in *patient_info* represent information about the patient like gender, age, location etc\n",
    "- The columns in *disorder_info* represent a series of boolean values that indicate whether the patient ails\n",
    "  from a particular disorder\n",
    "- The columns in *survey_info* represent the patient's responses to the BSC, GSC and LDS survey questions(assuming they are surveys)\n",
    "- The columns in *RCBF_raw* represent the raw cerebral blood flow values\n",
    "- The columns in *RCBF_scaled* represent the scaled cerebral blood flow values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
